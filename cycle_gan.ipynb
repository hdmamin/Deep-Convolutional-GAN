{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T17:49:50.309739Z",
     "start_time": "2019-06-11T17:49:50.259666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T17:49:52.900825Z",
     "start_time": "2019-06-11T17:49:50.410817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import cv2\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from config import *\n",
    "from htools import hdir\n",
    "from models import BaseModel, conv_block, Discriminator, GRelu, JRelu\n",
    "from torch_datasets import sketch_dl, photo_dl\n",
    "from utils import render_samples, show_img, show_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T17:49:55.385365Z",
     "start_time": "2019-06-11T17:49:55.338624Z"
    }
   },
   "outputs": [],
   "source": [
    "# NO NEED TO RUN, IMPORTED FROM CONFIG - JUST FOR EASY REFERENCE\n",
    "\n",
    "bs = 64                # Batch size (paper uses 128).\n",
    "img_size = 64          # Size of input (here it's 64 x 64).\n",
    "workers = 2            # Number of workers for data loader.\n",
    "input_c = 100          # Depth of input noise (1 x 1 x noise_dim). AKA nz.\n",
    "ngf = 64               # Filters in first G layer.\n",
    "ndf = 64               # Filters in first D layer.\n",
    "lr = 2e-4              # Recommended learning rate of .0002.\n",
    "beta1 = .5             # Recommended parameter for Adam.\n",
    "nc = 3                 # Number of channels of input image.\n",
    "ngpu = 1               # Number of GPUs to use.\n",
    "sample_dir = 'samples' # Directory to store sample images from G. \n",
    "weight_dir = 'weights' # Directory to store model weights.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() and ngpu > 0 \n",
    "                      else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T18:28:37.507975Z",
     "start_time": "2019-06-11T18:28:37.446175Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"Residual block to be used in CycleGenerator. Note that the relu or \n",
    "    leaky must still be applied on the output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, c_in, activation=nn.LeakyReLU(.02), num_layers=2, norm='bn'):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -----------\n",
    "        c_in: int\n",
    "            # of input channels.\n",
    "        num_layers: int\n",
    "            Number of conv blocks inside the skip connection (default 2). \n",
    "            ResNet paper notes that skipping a single layer did not show\n",
    "            noticeable improvements.\n",
    "        leak: float\n",
    "            Slope of leaky relu where x < 0.\n",
    "        norm: str\n",
    "            'bn' for batch norm, 'in' for instance norm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([conv_block(False, c_in, c_in, 3, 1, 1, norm=norm) \n",
    "                                     for i in range(num_layers)])\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_out = x\n",
    "        for layer in self.layers:\n",
    "            x_out = self.activation(layer(x_out))\n",
    "        return x + x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T18:28:32.243092Z",
     "start_time": "2019-06-11T18:28:32.123529Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLD\n",
    "# class ResBlock(nn.Module):\n",
    "#     \"\"\"Residual block to be used in CycleGenerator. Note that the relu or \n",
    "#     leaky must still be applied on the output.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, c_in, num_layers=2, leak=.02, norm='bn'):\n",
    "#         \"\"\"\n",
    "#         Parameters\n",
    "#         -----------\n",
    "#         c_in: int\n",
    "#             # of input channels.\n",
    "#         num_layers: int\n",
    "#             Number of conv blocks inside the skip connection (default 2). \n",
    "#             ResNet paper notes that skipping a single layer did not show\n",
    "#             noticeable improvements.\n",
    "#         leak: float\n",
    "#             Slope of leaky relu where x < 0.\n",
    "#         norm: str\n",
    "#             'bn' for batch norm, 'in' for instance norm\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.leak = leak\n",
    "#         self.layers = nn.ModuleList([conv_block(False, c_in, c_in, 3, 1, 1, norm=norm) \n",
    "#                                      for i in range(num_layers)])\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x_out = x\n",
    "#         for layer in self.layers:\n",
    "#             x_out = F.leaky_relu(layer(x_out), self.leak)\n",
    "#         return x + x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T18:25:54.354061Z",
     "start_time": "2019-06-11T18:25:53.965747Z"
    }
   },
   "outputs": [],
   "source": [
    "class CycleGenerator(BaseModel):\n",
    "    \"\"\"CycleGAN Generator.\"\"\"\n",
    "\n",
    "    def __init__(self, img_c=3, ngf=64, norm='bn', act=GRelu(.02)):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -----------\n",
    "        img_c: int\n",
    "            # of channels of input image.\n",
    "        ngf: int\n",
    "            # of channels in first convolutional layer.\n",
    "        norm: str\n",
    "            Type of normalization layer used for res blocks in the \n",
    "            transformer. Default is 'bn' for batch norm, but can also use\n",
    "            'in' for instance norm.\n",
    "        act: nn.Module\n",
    "            Default activation of GRelu(.02) gives us a leaky relu with a leak\n",
    "            of .02, as recommended in the paper. JRelu (already instantiated \n",
    "            so exclude parentheses) gives a leak of .1, sub of .4, and max of \n",
    "            6.0. GRelu() gives a standard ReLU.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.activation = act\n",
    "\n",
    "        # ENCODER\n",
    "        # 3 x 64 x 64 -> 64 x 32 x 32\n",
    "        deconv1 = conv_block(True, img_c, ngf, f=4, stride=2, pad=1)\n",
    "        # 64 x 32 x 32 -> 128 x 16 x 16\n",
    "        deconv2 = conv_block(True, ngf, ngf*2, 4, 2, 1)\n",
    "        self.encoder = nn.Sequential(deconv1, \n",
    "                                     self.activation,\n",
    "                                     deconv2,\n",
    "                                     self.activation)\n",
    "\n",
    "        # TRANSFORMER\n",
    "        # 128 x 16 x 16 -> 128 x 16 x 16\n",
    "        res1 = ResBlock(ngf*2, self.activation, num_layers=2, norm=norm)\n",
    "        # 128 x 16 x 16 -> 128 x 16 x 16\n",
    "        res2 = ResBlock(ngf*2, self.activation, 2, norm)\n",
    "        self.transformer = nn.Sequential(res1,\n",
    "                                         self.activation,\n",
    "                                         res2,\n",
    "                                         self.activation)\n",
    "\n",
    "        # DECODER\n",
    "        # 128 x 16 x 16 -> 64 x 32 x 32\n",
    "        deconv1 = conv_block(False, ngf*2, ngf, f=4, stride=2, pad=1)\n",
    "        # 64 x 32 x 32 -> 3 x 64 x 64\n",
    "        deconv2 = conv_block(False, ngf, img_c, 4, 2, 1)\n",
    "        self.decoder = nn.Sequential(deconv1, \n",
    "                                     self.activation,\n",
    "                                     deconv2,\n",
    "                                     nn.Tanh())\n",
    "\n",
    "        # Module list of Sequential objects is helpful if we want to use \n",
    "        # different learning rates per group.\n",
    "        self.groups = nn.ModuleList([self.encoder,\n",
    "                                     self.transformer,\n",
    "                                     self.decoder])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for group in self.groups:\n",
    "            x = group(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T06:06:09.073410Z",
     "start_time": "2019-06-11T06:06:09.053225Z"
    }
   },
   "outputs": [],
   "source": [
    "# class ResNetDiscriminator(BaseModel):\n",
    "    \n",
    "#     def __init__(self, img_c=3, ndf=64):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = conv_block(False, img_c, ndf, f=4, stride=2, pad=1)\n",
    "        \n",
    "#     def forward(self):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check - photo and sketch dataloaders may not always have same batch size?\n",
    "-also check: can we use class labels, not just 1's and 0's?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T07:04:10.945960Z",
     "start_time": "2019-06-11T07:04:10.913065Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_cycle_gan(epochs, x_dl, y_dl, sample_dir_x, sample_dir_y, \n",
    "                    weight_dir=None, sample_freq=10, lr=2e-4, b1=.5,\n",
    "                    use_labels=False, quiet_mode=True, load_path=None,\n",
    "                    models=None):\n",
    "    \"\"\"Train cycleGAN with Adam optimizer. The naming conventin G_xy will be\n",
    "    used to refer to a generator that converts from set x to set y, while\n",
    "    D_x refers to a discriminator that classifies examples as actually \n",
    "    belonging to set x (class 1) or being a model-generated example (class 0).\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    use_labels: bool\n",
    "        Specifies whether to use class labels (i.e. horse, zebra, giraffe). If\n",
    "        False, D only tries to predict if it is a real or fake example \n",
    "        (e.g. photo or sketch2photo).\n",
    "    load_path: str or None\n",
    "        If str, models will load state dicts from the provided file. If None,\n",
    "        new models will be created.\n",
    "    models: list or None\n",
    "        Instead of providing a load path, we can also pass in a list of models\n",
    "        in the form [G_xy, G_yx, D_x, D_y]. Either load_path or models \n",
    "        (or both) should be None.\n",
    "    \"\"\"\n",
    "    # Create models.\n",
    "    if not models:\n",
    "        G_xy, G_yx, D_x, D_y = get_cycle_models(load_path)\n",
    "    else:\n",
    "        for model in models:\n",
    "            model.to(device)\n",
    "            model.train()\n",
    "        G_xy, G_yx, D_x, D_y = models\n",
    "    \n",
    "    # Create optimizers.\n",
    "    optim_g = torch.optim.Adam(chain(G_xy.parameters(), G_yx.parameters()), \n",
    "                               lr, betas=(b1, .999))\n",
    "    optim_d = torch.optim.Adam(chain(D_x.parameters(), D_y.parameters()),\n",
    "                               lr, betas=(b1, .999))    \n",
    "    \n",
    "    # Define loss function.\n",
    "    if use_labels:\n",
    "        criterion = nn.BCELoss(reduction='mean')\n",
    "    else:\n",
    "        criterion = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "    # Set fixed examples for sample generation.\n",
    "    fixed_x = next(iter(x_dl))[0]\n",
    "    fixed_y = next(iter(y_dl))[0]\n",
    "    \n",
    "    # Suppress printed output to 20 total times to avoid slowing down nb.\n",
    "    print_freq = 1\n",
    "    if quiet_mode:\n",
    "        print_freq = max(1, epochs // 20)\n",
    "    \n",
    "    # Store lists of losses.\n",
    "    stats = defaultdict(list)\n",
    "    for epoch in range(epochs):\n",
    "        G_xy.train()\n",
    "        G_yx.train()\n",
    "        D_x.train()\n",
    "        D_y.train()\n",
    "        \n",
    "        for i, ((x, x_labels), (y, y_labels)) in enumerate(zip(x_dl, y_dl)):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_len = x.shape[0]\n",
    "            # X has less data so DL runs out and last batch is small. FIX?\n",
    "            if batch_len != y.shape[0]:\n",
    "                continue\n",
    "            labels_real = torch.ones(batch_len, device=device)\n",
    "            labels_fake = torch.zeros(batch_len, device=device)\n",
    "            \n",
    "            ##################################################################\n",
    "            # Train D_x and D_y.\n",
    "            ##################################################################\n",
    "            # Train D's on real images.\n",
    "            optim_d.zero_grad()\n",
    "            pred_x, pred_y = D_x(x), D_y(y)\n",
    "            loss_dx = criterion(pred_x, labels_real)\n",
    "            loss_dy = criterion(pred_y, labels_real)\n",
    "            loss_d_real = loss_dx + loss_dy\n",
    "            stats['d_real'].append(loss_d_real.item())\n",
    "            loss_d_real.backward()\n",
    "            optim_d.step()\n",
    "            \n",
    "            # Train D's on fake images.\n",
    "            optim_d.zero_grad()\n",
    "            x_fake, y_fake = G_yx(y), G_xy(x)\n",
    "            pred_x, pred_y = D_x(x_fake), D_y(y_fake)\n",
    "            loss_dx_fake = criterion(pred_x, labels_fake)\n",
    "            loss_dy_fake = criterion(pred_y, labels_fake)\n",
    "            loss_d_fake = loss_dx_fake + loss_dy_fake\n",
    "            stats['d_fake'].append(loss_d_fake.item())\n",
    "            loss_d_fake.backward()\n",
    "            optim_d.step()\n",
    "            \n",
    "            ##################################################################\n",
    "            # Train G_xy and G_yx.\n",
    "            ################################################################## \n",
    "            # Stage 1: x -> y -> x\n",
    "            optim_g.zero_grad()\n",
    "            y_fake = G_xy(x)\n",
    "            pred_y = D_y(y_fake)\n",
    "            loss_g = criterion(pred_y, labels_real)\n",
    "            \n",
    "            x_recon = G_yx(y_fake)\n",
    "            pred_x = D_x(x_recon)\n",
    "            loss_g_cycle = criterion(pred_x, labels_real)\n",
    "            loss_g_total = loss_g + loss_g_cycle\n",
    "            stats['g_xyx'].append(loss_g_total.item())\n",
    "            loss_g_total.backward()\n",
    "            optim_g.step()\n",
    "            \n",
    "            # Stage2: y -> x -> y\n",
    "            optim_g.zero_grad()\n",
    "            x_fake = G_yx(y)\n",
    "            pred_x = D_x(x_fake)\n",
    "            loss_g = criterion(pred_x, labels_real)\n",
    "            \n",
    "            y_recon = G_xy(x_fake)\n",
    "            pred_y = D_y(y_recon)\n",
    "            loss_g_cycle = criterion(pred_y, labels_real)\n",
    "            loss_g_total = loss_g + loss_g_cycle\n",
    "            stats['g_yxy'].append(loss_g_total.item())\n",
    "            loss_g_total.backward()\n",
    "            optim_g.step()\n",
    "        \n",
    "        # Generate samples to save at specified intervals.\n",
    "        if epoch % sample_freq == 0:\n",
    "            sample_x = G_yx(y).detach()\n",
    "            sample_y = G_yx(x).detach()\n",
    "            vutils.save_image(sample_x, f'{sample_dir_x}/{epoch}.png')\n",
    "            vutils.save_image(sample_y, f'{sample_dir_y}/{epoch}.png')\n",
    "            \n",
    "       # If specified, save weights corresponding to generated samples.\n",
    "        if weight_dir:\n",
    "            states = dict(g_xy=G_xy.state_dict(),\n",
    "                          g_yx=G_yx.state_dict(),\n",
    "                          dx=D_x.state_dict(),\n",
    "                          dy=D_y.state_dict(),\n",
    "                          epoch=epoch)\n",
    "            torch.save(states, f'{weight_dir}/{epoch}.pth') \n",
    "                \n",
    "        # Print results for last mini batch of epoch.\n",
    "        if epoch % print_freq == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}])')\n",
    "            print(f\"D real loss: {loss_d_real:.3f}\\t\"\n",
    "                  f\"D fake loss: {loss_d_fake:.3f}\")\n",
    "            print(f\"G xyx loss: {stats['g_xyx'][-1]:.3f}\\t\"\n",
    "                  f\"G yxy fake: {stats['g_yxy'][-1]:.3f}\\n\")\n",
    "            \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T06:26:48.892601Z",
     "start_time": "2019-06-11T06:26:48.869767Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cycle_models(path=None):\n",
    "    \"\"\"Get 2 cycle generators and 2 discriminators. If a path to a weight file\n",
    "    is provided, load state dicts from that file. Models are returned in train\n",
    "    mode.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    path: str\n",
    "        Optional - pass in path to weights file to load previously saved state\n",
    "        dicts, or exclude to get new models.\n",
    "    \"\"\"\n",
    "    models = dict(g_xy=CycleGenerator(), g_yx=CycleGenerator(), \n",
    "                  d_x=Discriminator(), d_y=Discriminator())\n",
    "    if path:\n",
    "        states = torch.load(path)\n",
    "        print(f\"Loading models from epoch {states['epoch']}.\")\n",
    "    for name, model in models.items():\n",
    "        if path:\n",
    "            model.load_state_dict(states[name])\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "    print('All models currently in training mode.')\n",
    "    return list(models.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T07:00:56.940468Z",
     "start_time": "2019-06-11T06:32:30.755991Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models currently in training mode.\n",
      "Epoch [1/60])\n",
      "D real loss: 0.000\tD loss fake: 0.000\n",
      "G xyx loss: 0.999\tG yxy fake: 1.000\n",
      "Epoch [4/60])\n",
      "D real loss: 0.000\tD loss fake: 0.000\n",
      "G xyx loss: 1.000\tG yxy fake: 1.000\n",
      "Epoch [7/60])\n",
      "D real loss: 0.000\tD loss fake: 0.000\n",
      "G xyx loss: 1.000\tG yxy fake: 1.000\n",
      "Epoch [10/60])\n",
      "D real loss: 0.000\tD loss fake: 1.000\n",
      "G xyx loss: 1.000\tG yxy fake: 1.000\n",
      "Epoch [13/60])\n",
      "D real loss: 0.000\tD loss fake: 0.000\n",
      "G xyx loss: 1.000\tG yxy fake: 2.000\n",
      "Epoch [16/60])\n",
      "D real loss: 0.000\tD loss fake: 0.000\n",
      "G xyx loss: 1.000\tG yxy fake: 2.000\n",
      "Epoch [19/60])\n",
      "D real loss: 0.000\tD loss fake: 0.000\n",
      "G xyx loss: 1.001\tG yxy fake: 2.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b649725484b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m stats = train_cycle_gan(60, photo_dl, sketch_dl, 'samples_cycle_photo', \n\u001b[0;32m----> 2\u001b[0;31m                         'samples_cycle_sketch')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-97d6c2ccbdee>\u001b[0m in \u001b[0;36mtrain_cycle_gan\u001b[0;34m(epochs, x_dl, y_dl, sample_dir_x, sample_dir_y, weight_dir, sample_freq, lr, b1, use_labels, quiet_mode, load_path, models)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0my_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mloss_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mx_recon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_yx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2255\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2256\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2257\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stats = train_cycle_gan(60, photo_dl, sketch_dl, 'samples_cycle_photo', \n",
    "                        'samples_cycle_sketch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:09:03.500347Z",
     "start_time": "2019-06-09T21:09:03.371769Z"
    }
   },
   "outputs": [],
   "source": [
    "G_xy = CycleGenerator(img_c, ngf)\n",
    "G_yx = CycleGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:08:12.090798Z",
     "start_time": "2019-06-09T21:08:11.982648Z"
    }
   },
   "outputs": [],
   "source": [
    "D = Discriminator(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:08:24.031139Z",
     "start_time": "2019-06-09T21:08:23.959237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:09:47.718202Z",
     "start_time": "2019-06-09T21:09:47.630443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = G_xy(x)\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:09:44.428977Z",
     "start_time": "2019-06-09T21:09:44.338454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_yx(y_hat).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
