{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T21:53:03.380459Z",
     "start_time": "2019-06-16T21:53:03.226601Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T21:54:40.485422Z",
     "start_time": "2019-06-16T21:54:40.464941Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import cv2\n",
    "import gc\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from config import *\n",
    "from htools import hdir\n",
    "from models import (conv_block, ResBlock, GRelu, JRelu, BaseModel,\n",
    "                    CycleGenerator, Discriminator)\n",
    "from torch_datasets import sketch_dl, photo_dl\n",
    "from utils import render_samples, show_img, show_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T17:49:55.385365Z",
     "start_time": "2019-06-11T17:49:55.338624Z"
    }
   },
   "outputs": [],
   "source": [
    "# # NO NEED TO RUN, IMPORTED FROM CONFIG - JUST FOR EASY REFERENCE\n",
    "\n",
    "# bs = 64                # Batch size (paper uses 128).\n",
    "# img_size = 64          # Size of input (here it's 64 x 64).\n",
    "# workers = 2            # Number of workers for data loader.\n",
    "# input_c = 100          # Depth of input noise (1 x 1 x noise_dim). AKA nz.\n",
    "# ngf = 64               # Filters in first G layer.\n",
    "# ndf = 64               # Filters in first D layer.\n",
    "# lr = 2e-4              # Recommended learning rate of .0002.\n",
    "# beta1 = .5             # Recommended parameter for Adam.\n",
    "# nc = 3                 # Number of channels of input image.\n",
    "# ngpu = 1               # Number of GPUs to use.\n",
    "# sample_dir = 'samples' # Directory to store sample images from G. \n",
    "# weight_dir = 'weights' # Directory to store model weights.\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() and ngpu > 0 \n",
    "#                       else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T21:54:09.582988Z",
     "start_time": "2019-06-16T21:54:09.561754Z"
    }
   },
   "outputs": [],
   "source": [
    "# class ResBlock(nn.Module):\n",
    "#     \"\"\"Residual block to be used in CycleGenerator. Note that the relu or \n",
    "#     leaky must still be applied on the output.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, c_in, activation=GRelu(.02), num_layers=2, norm='bn'):\n",
    "#         \"\"\"\n",
    "#         Parameters\n",
    "#         -----------\n",
    "#         c_in: int\n",
    "#             # of input channels.\n",
    "#         num_layers: int\n",
    "#             Number of conv blocks inside the skip connection (default 2). \n",
    "#             ResNet paper notes that skipping a single layer did not show\n",
    "#             noticeable improvements.\n",
    "#         leak: float\n",
    "#             Slope of leaky relu where x < 0.\n",
    "#         norm: str\n",
    "#             'bn' for batch norm, 'in' for instance norm\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList([conv_block(False, c_in, c_in, 3, 1, 1, norm=norm) \n",
    "#                                      for i in range(num_layers)])\n",
    "#         self.activation = activation\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x_out = x\n",
    "#         for layer in self.layers:\n",
    "#             x_out = self.activation(layer(x_out))\n",
    "#         return x + x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T00:59:01.134879Z",
     "start_time": "2019-06-14T00:59:01.039503Z"
    }
   },
   "outputs": [],
   "source": [
    "# class CycleGenerator(BaseModel):\n",
    "#     \"\"\"CycleGAN Generator.\"\"\"\n",
    "\n",
    "#     def __init__(self, img_c=3, ngf=64, norm='bn', act=GRelu(.02)):\n",
    "#         \"\"\"\n",
    "#         Parameters\n",
    "#         -----------\n",
    "#         img_c: int\n",
    "#             # of channels of input image.\n",
    "#         ngf: int\n",
    "#             # of channels in first convolutional layer.\n",
    "#         norm: str\n",
    "#             Type of normalization layer used for res blocks in the \n",
    "#             transformer. Default is 'bn' for batch norm, but can also use\n",
    "#             'in' for instance norm.\n",
    "#         act: nn.Module\n",
    "#             Default activation of GRelu(.02) gives us a leaky relu with a leak\n",
    "#             of .02, as recommended in the paper. JRelu (already instantiated \n",
    "#             so exclude parentheses) gives a leak of .1, sub of .4, and max of \n",
    "#             6.0. GRelu() gives a standard ReLU.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.activation = act\n",
    "\n",
    "#         # ENCODER\n",
    "#         # 3 x 64 x 64 -> 64 x 32 x 32\n",
    "#         deconv1 = conv_block(True, img_c, ngf, f=4, stride=2, pad=1)\n",
    "#         # 64 x 32 x 32 -> 128 x 16 x 16\n",
    "#         deconv2 = conv_block(True, ngf, ngf*2, 4, 2, 1)\n",
    "#         self.encoder = nn.Sequential(deconv1, \n",
    "#                                      self.activation,\n",
    "#                                      deconv2,\n",
    "#                                      self.activation)\n",
    "\n",
    "#         # TRANSFORMER\n",
    "#         # 128 x 16 x 16 -> 128 x 16 x 16\n",
    "#         res1 = ResBlock(ngf*2, self.activation, num_layers=2, norm=norm)\n",
    "#         # 128 x 16 x 16 -> 128 x 16 x 16\n",
    "#         res2 = ResBlock(ngf*2, self.activation, 2, norm)\n",
    "#         self.transformer = nn.Sequential(res1,\n",
    "#                                          self.activation,\n",
    "#                                          res2,\n",
    "#                                          self.activation)\n",
    "\n",
    "#         # DECODER\n",
    "#         # 128 x 16 x 16 -> 64 x 32 x 32\n",
    "#         deconv1 = conv_block(False, ngf*2, ngf, f=4, stride=2, pad=1)\n",
    "#         # 64 x 32 x 32 -> 3 x 64 x 64\n",
    "#         deconv2 = conv_block(False, ngf, img_c, 4, 2, 1)\n",
    "#         self.decoder = nn.Sequential(deconv1, \n",
    "#                                      self.activation,\n",
    "#                                      deconv2,\n",
    "#                                      nn.Tanh())\n",
    "\n",
    "#         # Module list of Sequential objects is helpful if we want to use \n",
    "#         # different learning rates per group.\n",
    "#         self.groups = nn.ModuleList([self.encoder,\n",
    "#                                      self.transformer,\n",
    "#                                      self.decoder])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for group in self.groups:\n",
    "#             x = group(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDEA: maybe we can use 1 discriminator with output classes real x, real y, fake x, fake y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T21:54:56.774455Z",
     "start_time": "2019-06-16T21:54:56.750598Z"
    }
   },
   "outputs": [],
   "source": [
    "class PretrainedDiscriminator(BaseModel):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layers = list(resnet18(pretrained=True).children())[:-2]\n",
    "#       # WAS THINKING OF W/ REPLACING RELU W/ GRELU; BIT TRICKY W/ SEQUENTIAL\n",
    "#         self.act = act\n",
    "#         for layer in layers:\n",
    "#             if layer.__class__.__name__ == 'ReLU':\n",
    "#                 layer = self.act\n",
    "        self.groups = nn.ModuleList([nn.Sequential(*layers[:6]),\n",
    "                                     nn.Sequential(*layers[6:])])\n",
    "        self.freeze()\n",
    "        self.groups.append(nn.Linear(512, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for group in self.groups[:-1]:\n",
    "            x = group(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = self.groups[-1](x.view(x.shape[0], -1))\n",
    "        return torch.sigmoid(x).squeeze()\n",
    "    \n",
    "    def freeze(self):\n",
    "        \"\"\"Freeze the whole network.\"\"\"\n",
    "        for group in self.groups:\n",
    "            for p in group.parameters():\n",
    "                p.requires_grad = False\n",
    "                \n",
    "    def unfreeze(self, i=None, verbose=False):\n",
    "        \"\"\"Unfreeze all or part of the network.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        i: None or int\n",
    "            If None, unfreeze the whole network. If int, unfreeze only that\n",
    "            group.\n",
    "        \"\"\"\n",
    "        for j, group in enumerate(self.groups):\n",
    "            if i is None or j == i:\n",
    "                if verbose: print(f'Unfreezing group {j}.')\n",
    "                for p in group.parameters():\n",
    "                    p.requires_grad = True\n",
    "            else:\n",
    "                if verbose: print(f'Freezing group {j}.')\n",
    "                for p in group.parameters():\n",
    "                    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:10:29.304071Z",
     "start_time": "2019-06-14T01:10:28.701834Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(photo_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:10:30.199220Z",
     "start_time": "2019-06-14T01:10:29.307705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 64, 64])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = CycleGenerator()\n",
    "y_fake = G(x)\n",
    "y_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:13:22.811448Z",
     "start_time": "2019-06-14T01:13:22.027717Z"
    }
   },
   "outputs": [],
   "source": [
    "D = PretrainedDiscriminator()\n",
    "pred_y_fake = D(y_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check - photo and sketch dataloaders may not always have same batch size?\n",
    "-also check: can we use class labels, not just 1's and 0's?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T21:56:20.960017Z",
     "start_time": "2019-06-16T21:56:20.923930Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_cycle_gan(epochs, x_dl, y_dl, sample_dir_x, sample_dir_y, \n",
    "                    weight_dir=None, sample_freq=10, lr=2e-4, b1=.5,\n",
    "                    use_labels=False, quiet_mode=True, load_path=None,\n",
    "                    models=None):\n",
    "    \"\"\"Train cycleGAN with Adam optimizer. The naming conventin G_xy will be\n",
    "    used to refer to a generator that converts from set x to set y, while\n",
    "    D_x refers to a discriminator that classifies examples as actually \n",
    "    belonging to set x (class 1) or being a model-generated example (class 0).\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    use_labels: bool\n",
    "        Specifies whether to use class labels (i.e. horse, zebra, giraffe). If\n",
    "        False, D only tries to predict if it is a real or fake example \n",
    "        (e.g. photo or sketch2photo).\n",
    "    load_path: str or None\n",
    "        If str, models will load state dicts from the provided file. If None,\n",
    "        new models will be created.\n",
    "    models: list or None\n",
    "        Instead of providing a load path, we can also pass in a list of models\n",
    "        in the form [G_xy, G_yx, D_x, D_y]. Either load_path or models \n",
    "        (or both) should be None.\n",
    "    \"\"\"\n",
    "    # Create models.\n",
    "    if not models:\n",
    "        G_xy, G_yx, D_x, D_y = get_cycle_models(load_path)\n",
    "    else:\n",
    "        for model in models:\n",
    "            model.to(device)\n",
    "            model.train()\n",
    "        G_xy, G_yx, D_x, D_y = models\n",
    "    \n",
    "    # Create optimizers.\n",
    "    optim_g = torch.optim.Adam(chain(G_xy.parameters(), G_yx.parameters()), \n",
    "                               lr, betas=(b1, .999))\n",
    "    optim_d = torch.optim.Adam(chain(D_x.parameters(), D_y.parameters()),\n",
    "                               lr, betas=(b1, .999))    \n",
    "    \n",
    "    # Define loss function.\n",
    "    if use_labels:\n",
    "        criterion = nn.BCELoss(reduction='mean')\n",
    "    else:\n",
    "        criterion = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "    # Set fixed examples for sample generation.\n",
    "    fixed_x = next(iter(x_dl))[0]\n",
    "    fixed_y = next(iter(y_dl))[0]\n",
    "    \n",
    "    # Suppress printed output to 20 total times to avoid slowing down nb.\n",
    "    print_freq = 1\n",
    "    if quiet_mode:\n",
    "        print_freq = max(1, epochs // 20)\n",
    "    \n",
    "    # Store lists of losses.\n",
    "    stats = defaultdict(list)\n",
    "    for epoch in range(epochs):\n",
    "        G_xy.train()\n",
    "        G_yx.train()\n",
    "        D_x.train()\n",
    "        D_y.train()\n",
    "        \n",
    "        for i, ((x, x_labels), (y, y_labels)) in enumerate(zip(x_dl, y_dl)):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_len = x.shape[0]\n",
    "            # X has less data so DL runs out and last batch is small. FIX?\n",
    "            if batch_len != y.shape[0]:\n",
    "                continue\n",
    "            labels_real = torch.ones(batch_len, device=device)\n",
    "            labels_fake = torch.zeros(batch_len, device=device)\n",
    "            \n",
    "            ##################################################################\n",
    "            # Train D_x and D_y.\n",
    "            ##################################################################\n",
    "            # Train D's on real images.\n",
    "            optim_d.zero_grad()\n",
    "            pred_x, pred_y = D_x(x), D_y(y)\n",
    "            loss_dx = criterion(pred_x, labels_real)\n",
    "            loss_dy = criterion(pred_y, labels_real)\n",
    "            loss_d_real = loss_dx + loss_dy\n",
    "            stats['d_real'].append(loss_d_real.item())\n",
    "            loss_d_real.backward()\n",
    "            optim_d.step()\n",
    "            \n",
    "            # Train D's on fake images.\n",
    "            optim_d.zero_grad()\n",
    "            x_fake, y_fake = G_yx(y), G_xy(x)\n",
    "            pred_x, pred_y = D_x(x_fake), D_y(y_fake)\n",
    "            loss_dx_fake = criterion(pred_x, labels_fake)\n",
    "            loss_dy_fake = criterion(pred_y, labels_fake)\n",
    "            loss_d_fake = loss_dx_fake + loss_dy_fake\n",
    "            stats['d_fake'].append(loss_d_fake.item())\n",
    "            loss_d_fake.backward()\n",
    "            optim_d.step()\n",
    "            \n",
    "            ##################################################################\n",
    "            # Train G_xy and G_yx.\n",
    "            ################################################################## \n",
    "            # Stage 1: x -> y -> x\n",
    "            optim_g.zero_grad()\n",
    "            y_fake = G_xy(x)\n",
    "            pred_y = D_y(y_fake)\n",
    "            loss_g = criterion(pred_y, labels_real)\n",
    "            \n",
    "            x_recon = G_yx(y_fake)\n",
    "            pred_x = D_x(x_recon)\n",
    "            loss_g_cycle = criterion(pred_x, labels_real)\n",
    "            loss_g_total = loss_g + loss_g_cycle\n",
    "            stats['g_xyx'].append(loss_g_total.item())\n",
    "            loss_g_total.backward()\n",
    "            optim_g.step()\n",
    "            \n",
    "            # Stage2: y -> x -> y\n",
    "            optim_g.zero_grad()\n",
    "            x_fake = G_yx(y)\n",
    "            pred_x = D_x(x_fake)\n",
    "            loss_g = criterion(pred_x, labels_real)\n",
    "            \n",
    "            y_recon = G_xy(x_fake)\n",
    "            pred_y = D_y(y_recon)\n",
    "            loss_g_cycle = criterion(pred_y, labels_real)\n",
    "            loss_g_total = loss_g + loss_g_cycle\n",
    "            stats['g_yxy'].append(loss_g_total.item())\n",
    "            loss_g_total.backward()\n",
    "            optim_g.step()\n",
    "        \n",
    "        # Generate samples to save at specified intervals.\n",
    "        if epoch % sample_freq == 0:\n",
    "            sample_x = G_yx(y).detach()\n",
    "            sample_y = G_yx(x).detach()\n",
    "            vutils.save_image(sample_x, f'{sample_dir_x}/{epoch}.png')\n",
    "            vutils.save_image(sample_y, f'{sample_dir_y}/{epoch}.png')\n",
    "            \n",
    "       # If specified, save weights corresponding to generated samples.\n",
    "        if weight_dir:\n",
    "            states = dict(g_xy=G_xy.state_dict(),\n",
    "                          g_yx=G_yx.state_dict(),\n",
    "                          dx=D_x.state_dict(),\n",
    "                          dy=D_y.state_dict(),\n",
    "                          epoch=epoch)\n",
    "            torch.save(states, f'{weight_dir}/{epoch}.pth') \n",
    "                \n",
    "        # Print results for last mini batch of epoch.\n",
    "        if epoch % print_freq == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}])')\n",
    "            print(f\"D real loss: {loss_d_real:.3f}\\t\"\n",
    "                  f\"D fake loss: {loss_d_fake:.3f}\")\n",
    "            print(f\"G xyx loss: {stats['g_xyx'][-1]:.3f}\\t\"\n",
    "                  f\"G yxy fake: {stats['g_yxy'][-1]:.3f}\\n\")\n",
    "            \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T21:56:54.964160Z",
     "start_time": "2019-06-16T21:56:54.941542Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cycle_models(path=None):\n",
    "    \"\"\"Get 2 cycle generators and 2 discriminators. If a path to a weight file\n",
    "    is provided, load state dicts from that file. Models are returned in train\n",
    "    mode.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    path: str\n",
    "        Optional - pass in path to weights file to load previously saved state\n",
    "        dicts, or exclude to get new models.\n",
    "    \"\"\"\n",
    "    models = dict(g_xy=CycleGenerator(), g_yx=CycleGenerator(), \n",
    "                  d_x=Discriminator(), d_y=Discriminator())\n",
    "    if path:\n",
    "        states = torch.load(path)\n",
    "        print(f\"Loading models from epoch {states['epoch']}.\")\n",
    "    for name, model in models.items():\n",
    "        if path:\n",
    "            model.load_state_dict(states[name])\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "    print('All models currently in training mode.')\n",
    "    return list(models.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T21:57:04.162201Z",
     "start_time": "2019-06-16T21:57:00.917715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models currently in training mode.\n"
     ]
    }
   ],
   "source": [
    "m = get_cycle_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T22:06:51.825703Z",
     "start_time": "2019-06-16T21:59:09.870960Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models currently in training mode.\n",
      "Epoch [1/5])\n",
      "D real loss: 0.000\tD fake loss: 0.000\n",
      "G xyx loss: 1.000\tG yxy fake: 1.000\n",
      "\n",
      "Epoch [2/5])\n",
      "D real loss: 0.000\tD fake loss: 0.000\n",
      "G xyx loss: 1.000\tG yxy fake: 1.000\n",
      "\n",
      "Epoch [3/5])\n",
      "D real loss: 0.000\tD fake loss: 0.000\n",
      "G xyx loss: 1.000\tG yxy fake: 1.000\n",
      "\n",
      "Epoch [4/5])\n",
      "D real loss: 0.000\tD fake loss: 0.000\n",
      "G xyx loss: 1.000\tG yxy fake: 1.000\n",
      "\n",
      "Epoch [5/5])\n",
      "D real loss: 0.000\tD fake loss: 0.000\n",
      "G xyx loss: 1.000\tG yxy fake: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats = train_cycle_gan(5, photo_dl, sketch_dl, 'samples_cycle_photo', \n",
    "                        'samples_cycle_sketch', sample_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:09:03.500347Z",
     "start_time": "2019-06-09T21:09:03.371769Z"
    }
   },
   "outputs": [],
   "source": [
    "G_xy = CycleGenerator(img_c, ngf)\n",
    "G_yx = CycleGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:08:12.090798Z",
     "start_time": "2019-06-09T21:08:11.982648Z"
    }
   },
   "outputs": [],
   "source": [
    "D = Discriminator(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:08:24.031139Z",
     "start_time": "2019-06-09T21:08:23.959237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:09:47.718202Z",
     "start_time": "2019-06-09T21:09:47.630443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = G_xy(x)\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:09:44.428977Z",
     "start_time": "2019-06-09T21:09:44.338454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_yx(y_hat).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
