{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T23:47:37.748549Z",
     "start_time": "2019-06-08T23:47:37.639239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:31:32.323044Z",
     "start_time": "2019-06-09T21:31:32.248005Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import cv2\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from config import *\n",
    "from htools import hdir\n",
    "from models import BaseModel, conv_block, Discriminator\n",
    "from torch_datasets import sketch_dl, photo_dl\n",
    "from utils import render_samples, show_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T23:47:48.968222Z",
     "start_time": "2019-06-08T23:47:48.903165Z"
    }
   },
   "outputs": [],
   "source": [
    "# TEMPORARY, IMPORTED FROM CONFIG - JUST FOR EASY REFERENCE\n",
    "\n",
    "bs = 64                # Batch size (paper uses 128).\n",
    "img_size = 64          # Size of input (here it's 64 x 64).\n",
    "workers = 2            # Number of workers for data loader.\n",
    "input_c = 100          # Depth of input noise (1 x 1 x noise_dim). AKA nz.\n",
    "ngf = 64               # Filters in first G layer.\n",
    "ndf = 64               # Filters in first D layer.\n",
    "lr = 2e-4              # Recommended learning rate of .0002.\n",
    "beta1 = .5             # Recommended parameter for Adam.\n",
    "nc = 3                 # Number of channels of input image.\n",
    "ngpu = 1               # Number of GPUs to use.\n",
    "sample_dir = 'samples' # Directory to store sample images from G. \n",
    "weight_dir = 'weights' # Directory to store model weights.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() and ngpu > 0 \n",
    "                      else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T00:54:23.589354Z",
     "start_time": "2019-06-10T00:54:23.353534Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_block??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T06:05:35.051699Z",
     "start_time": "2019-06-09T06:05:34.930071Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"Residual block to be used in CycleGenerator. Note that the relu or \n",
    "    leaky must still be applied on the output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, c_in, num_layers=2, leak=.02):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -----------\n",
    "        c_in: int\n",
    "            # of input channels.\n",
    "        num_layers: int\n",
    "            Number of conv blocks inside the skip connection (default 2). \n",
    "            ResNet paper notes that skipping a single layer did not show\n",
    "            noticeable improvements.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.leak = leak\n",
    "        self.layers = nn.ModuleList([conv_block(False, c_in, c_in, 3, 1, 1) \n",
    "                                     for i in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_out = x\n",
    "        for layer in self.layers:\n",
    "            x_out = F.leaky_relu(layer(x_out), self.leak)\n",
    "        return x + x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T06:06:51.328997Z",
     "start_time": "2019-06-09T06:06:51.237472Z"
    }
   },
   "outputs": [],
   "source": [
    "class CycleGenerator(BaseModel):\n",
    "    \"\"\"CycleGAN Generator.\"\"\"\n",
    "\n",
    "    def __init__(self, img_c=3, ngf=64, leak=.02):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -----------\n",
    "        img_c: int\n",
    "            # of channels of input image.\n",
    "        ngf: int\n",
    "            # of channels in first convolutional layer.\n",
    "        leak: float\n",
    "            Slope of leaky relu where x < 0. Leak of 0 is regular relu.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.leak = leak\n",
    "        self.activation = nn.LeakyReLU(self.leak)\n",
    "\n",
    "        # ENCODER\n",
    "        # 3 x 64 x 64 -> 64 x 32 x 32\n",
    "        deconv1 = conv_block(False, img_c, ngf, f=4, stride=2, pad=1)\n",
    "        # 64 x 32 x 32 -> 128 x 16 x 16\n",
    "        deconv2 = conv_block(False, ngf, ngf*2, 4, 2, 1)\n",
    "        self.encoder = nn.Sequential(deconv1, \n",
    "                                     self.activation,\n",
    "                                     deconv2,\n",
    "                                     self.activation)\n",
    "\n",
    "        # TRANSFORMER\n",
    "        # 128 x 16 x 16 -> 128 x 16 x 16\n",
    "        res1 = ResBlock(ngf*2, num_layers=2, leak=self.leak)\n",
    "        # 128 x 16 x 16 -> 128 x 16 x 16\n",
    "        res2 = ResBlock(ngf*2, 2, self.leak)\n",
    "        self.transformer = nn.Sequential(res1,\n",
    "                                         self.activation,\n",
    "                                         res2,\n",
    "                                         self.activation)\n",
    "\n",
    "        # DECODER\n",
    "        # 128 x 16 x 16 -> 64 x 32 x 32\n",
    "        deconv1 = conv_block(True, ngf*2, ngf, f=4, stride=2, pad=1)\n",
    "        # 64 x 32 x 32 -> 3 x 64 x 64\n",
    "        deconv2 = conv_block(True, ngf, img_c, 4, 2, 1)\n",
    "        self.decoder = nn.Sequential(deconv1, \n",
    "                                     self.activation,\n",
    "                                     deconv2,\n",
    "                                     nn.Tanh())\n",
    "\n",
    "        # Module list of Sequential objects is helpful if we want to use \n",
    "        # different learning rates per group.\n",
    "        self.groups = nn.ModuleList([self.encoder,\n",
    "                                     self.transformer,\n",
    "                                     self.decoder])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for group in self.groups:\n",
    "            x = group(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T06:46:48.368852Z",
     "start_time": "2019-06-09T06:46:48.046949Z"
    }
   },
   "outputs": [],
   "source": [
    "# class ResNetDiscriminator(BaseModel):\n",
    "    \n",
    "#     def __init__(self, img_c=3, ndf=64):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = conv_block(False, img_c, ndf, f=4, stride=2, pad=1)\n",
    "        \n",
    "#     def forward(self):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check - photo and sketch dataloaders may not always have same batch size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T00:40:20.387376Z",
     "start_time": "2019-06-10T00:40:20.299650Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_cycle_gan(epochs, x_dl, y_dl, sample_dir_x, sample_dir_y, \n",
    "                    weight_dir=None, sample_freq=10, lr=2e-4, b1=.5,\n",
    "                    use_labels=False, load_path=None, models=None):\n",
    "    \"\"\"Train cycleGAN with Adam optimizer. The naming conventin G_xy will be\n",
    "    used to refer to a generator that converts from set x to set y, while\n",
    "    D_x refers to a discriminator that classifies examples as actually \n",
    "    belonging to set x (class 1) or being a model-generated example (class 0).\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    use_labels: bool\n",
    "        Specifies whether to use class labels (i.e. horse, zebra, giraffe). If\n",
    "        False, D only tries to predict if it is a real or fake example \n",
    "        (e.g. photo or sketch2photo).\n",
    "    load_path: str or None\n",
    "        If str, models will load state dicts from the provided file. If None,\n",
    "        new models will be created.\n",
    "    models: list or None\n",
    "        Instead of providing a load path, we can also pass in a list of models\n",
    "        in the form [G_xy, G_yx, D_x, D_y]. Either load_path or models \n",
    "        (or both) should be None.\n",
    "    \"\"\"\n",
    "    # Create models.\n",
    "    if not models:\n",
    "        G_xy, G_yx, D_x, D_y = get_cycle_models(load_path)\n",
    "    else:\n",
    "        for model in models:\n",
    "            model.to(device)\n",
    "            model.train()\n",
    "        G_xy, G_yx, D_x, D_y = models\n",
    "    \n",
    "    # Create optimizers.\n",
    "    optim_g = torch.optim.Adam(chain(G_xy.parameters(), G_yx.parameters()), \n",
    "                               lr, betas=(b1, .999))\n",
    "    optim_d = torch.optim.Adam(chain(D_x.parameters(), D_y.parameters()),\n",
    "                               lr, betas=(b1, .999))    \n",
    "    \n",
    "    # Define loss function.\n",
    "    if use_labels:\n",
    "        criterion = nn.BCELoss(reduction='mean')\n",
    "    else:\n",
    "        criterion = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "    # Set fixed examples for sample generation.\n",
    "    fixed_x = next(iter(x_dl))[0]\n",
    "    fixed_y = next(iter(y_dl))[0]\n",
    "    \n",
    "    stats = defaultdict()\n",
    "    for epoch in range(epochs):\n",
    "        G_xy.train()\n",
    "        G_yx.train()\n",
    "        D_x.train()\n",
    "        D_y.train()\n",
    "        \n",
    "        for i, ((x, x_labels), (y, y_labels)) in enumerate(zip(x_dl, y_dl)):\n",
    "            print(i)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_len = x.shape[0]\n",
    "            labels_real = torch.ones(batch_len, device=device)\n",
    "            labels_fake = torch.zeros(batch_len, device=device)\n",
    "            \n",
    "            print('got labels')\n",
    "            ##################################################################\n",
    "            # Train D_x and D_y.\n",
    "            ##################################################################\n",
    "            # Train D's on real images.\n",
    "            optim_d.zero_grad()\n",
    "            pred_x, pred_y = D_x(x), D_y(y)\n",
    "            loss_dx = criterion(pred_x, labels_real)\n",
    "            loss_dy = criterion(pred_y, labels_real)\n",
    "            loss_d_real = loss_dx + loss_dy\n",
    "            loss_d_real.backward()\n",
    "            optim_d.step()\n",
    "            print('first step')\n",
    "            \n",
    "            # Train D's on fake images.\n",
    "            optim_d.zero_grad()\n",
    "            x_fake, y_fake = G_yx(y), G_xy(x)\n",
    "            pred_x, pred_y = D_x(x_fake), D_y(y_fake)\n",
    "            loss_dx_fake = criterion(pred_x, labels_fake)\n",
    "            loss_dy_fake = criterion(pred_y, labels_fake)\n",
    "            loss_d_fake = loss_dx_fake + loss_dy_fake\n",
    "            loss_d_fake.backward()\n",
    "            optim_d.step()\n",
    "            print('second step')\n",
    "            \n",
    "            ##################################################################\n",
    "            # Train G_xy and G_yx.\n",
    "            ################################################################## \n",
    "            # Stage 1: x -> y -> x\n",
    "            optim_g.zero_grad()\n",
    "            y_fake = G_xy(x)\n",
    "            pred_y = D_y(y_fake)\n",
    "            loss_g = criterion(pred_y, labels_real)\n",
    "            \n",
    "            x_recon = G_yx(y_fake)\n",
    "            pred_x = D_x(x_recon)\n",
    "            loss_g_cycle = criterion(pred_x, labels_real)\n",
    "            loss_g_total = loss_g + loss_g_cycle\n",
    "            loss_g_total.backward()\n",
    "            optim_g.step()\n",
    "            \n",
    "            # Stage2: y -> x -> y\n",
    "            optim_g.zero_grad()\n",
    "            x_fake = G_yx(y)\n",
    "            pred_x = D_x(x_fake)\n",
    "            loss_g = criterion(pred_x, labels_real)\n",
    "            \n",
    "            y_recon = G_xy(x_fake)\n",
    "            pred_y = D_y(y_recon)\n",
    "            loss_g_cycle = criterion(pred_y, labels_real)\n",
    "            loss_g_total = loss_g + loss_g_cycle\n",
    "            loss_g_total.backward()\n",
    "            optim_g.step()\n",
    "        \n",
    "        # Generate samples to save at specified intervals.\n",
    "        if epoch % sample_freq == 0:\n",
    "            sample_x = G_yx(y).detach()\n",
    "            sample_y = G_yx(x).detach()\n",
    "            vutils.save_image(sample_x, f'{sample_dir_x}/{epoch}.png')\n",
    "            vutils.save_image(sample_y, f'{sample_dir_y}/{epoch}.png')\n",
    "            \n",
    "       # If specified, save weights corresponding to generated samples.\n",
    "        if weight_dir:\n",
    "            states = dict(g_xy=G_xy.state_dict(),\n",
    "                          g_yx=G_yx.state_dict(),\n",
    "                          dx=D_x.state_dict(),\n",
    "                          dy=D_y.state_dict(),\n",
    "                          epoch=epoch)\n",
    "            torch.save(states, f'{weight_dir}/{epoch}.pth') \n",
    "                \n",
    "        # Print results for last mini batch of epoch.\n",
    "        print(f'Epoch [{epoch+1}/{epochs}])')\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T00:40:13.369044Z",
     "start_time": "2019-06-10T00:40:13.297875Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cycle_models(path=None):\n",
    "    \"\"\"Get 2 cycle generators and 2 discriminators. If a path to a weight file\n",
    "    is provided, load state dicts from that file. Models are returned in train\n",
    "    mode.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    path: str\n",
    "        Optional - pass in path to weights file to load previously saved state\n",
    "        dicts, or exclude to get new models.\n",
    "    \"\"\"\n",
    "    models = dict(g_xy=CycleGenerator(), g_yx=CycleGenerator(), \n",
    "                  d_x=Discriminator(), d_y=Discriminator())\n",
    "    if path:\n",
    "        states = torch.load(path)\n",
    "        print(f\"Loading models from epoch {states['epoch']}.\")\n",
    "    for name, model in models.items():\n",
    "        if path:\n",
    "            model.load_state_dict(states[name])\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "    print('All models currently in training mode.')\n",
    "    return list(models.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T00:39:36.722456Z",
     "start_time": "2019-06-10T00:39:36.651033Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_cycle_gan(1, photo_dl, sketch_dl, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:09:03.500347Z",
     "start_time": "2019-06-09T21:09:03.371769Z"
    }
   },
   "outputs": [],
   "source": [
    "G_xy = CycleGenerator(img_c, ngf)\n",
    "G_yx = CycleGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:08:12.090798Z",
     "start_time": "2019-06-09T21:08:11.982648Z"
    }
   },
   "outputs": [],
   "source": [
    "D = Discriminator(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:08:24.031139Z",
     "start_time": "2019-06-09T21:08:23.959237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:09:47.718202Z",
     "start_time": "2019-06-09T21:09:47.630443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = G_xy(x)\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T21:09:44.428977Z",
     "start_time": "2019-06-09T21:09:44.338454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_yx(y_hat).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
